{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 0: Setup, Data Loading, EDA, and Cleaning\n",
    "\n",
    "This phase handles the initial setup by installing required packages, downloading the dataset from Kaggle, performing exploratory data analysis (EDA), cleaning and imputing missing values in the product and review datasets, and verifying the cleaned data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install chromadb sentence-transformers torch kaggle datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Kaggle API and download dataset\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Check if dataset already exists\n",
    "expected_files = [\n",
    "    'product_info.csv',\n",
    "    'reviews_0-250.csv',\n",
    "    'reviews_250-500.csv', \n",
    "    'reviews_500-750.csv',\n",
    "    'reviews_750-1250.csv',\n",
    "    'reviews_1250-end.csv'\n",
    "]\n",
    "\n",
    "if os.path.exists('./sephora_data') and all(os.path.exists(f'./sephora_data/{f}') for f in expected_files):\n",
    "    print(\"Dataset already exists, skipping download!\")\n",
    "else:\n",
    "    # Download dataset\n",
    "    load_dotenv()\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_files('nadyinky/sephora-products-and-skincare-reviews', path='./sephora_data', unzip=True)\n",
    "    print(\"Dataset downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial Exploratory Data Analysis (EDA)\n",
    "def initial_eda(df, name):\n",
    "    eda_summary = pd.DataFrame({\n",
    "        'Column': df.columns,\n",
    "        'Data Type': df.dtypes.values,\n",
    "        'Null Count': df.isnull().sum().values,\n",
    "        'Null %': (df.isnull().sum() / len(df) * 100).round(2).values\n",
    "    })\n",
    "    print(f\"EDA for {name}:\")\n",
    "    print(eda_summary.to_string(index=False))\n",
    "    print(f\"Shape: {df.shape[0]} rows × {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Ensure data is loaded as DataFrame\n",
    "try:\n",
    "    product_info_loaded = isinstance(product_info, pd.DataFrame)\n",
    "except NameError:\n",
    "    product_info_loaded = False\n",
    "\n",
    "if not product_info_loaded:\n",
    "    print(\"Loading product_info from CSV...\")\n",
    "    try:\n",
    "        product_info = pd.read_csv('./sephora_data/product_info.csv')\n",
    "        print(\"Product info loaded successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: product_info.csv not found. Please run the data download cell first.\")\n",
    "        product_info = pd.DataFrame()  # Empty DataFrame to avoid error\n",
    "\n",
    "try:\n",
    "    reviews_loaded = isinstance(reviews, pd.DataFrame)\n",
    "except NameError:\n",
    "    reviews_loaded = False\n",
    "\n",
    "if not reviews_loaded:\n",
    "    print(\"Loading reviews from CSV...\")\n",
    "    try:\n",
    "        review_files = ['./sephora_data/reviews_0-250.csv', './sephora_data/reviews_250-500.csv', './sephora_data/reviews_500-750.csv', './sephora_data/reviews_750-1250.csv', './sephora_data/reviews_1250-end.csv']\n",
    "        reviews = pd.concat([pd.read_csv(f) for f in review_files], ignore_index=True)\n",
    "        print(\"Reviews loaded successfully\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: review files not found. Please run the data download cell first.\")\n",
    "        reviews = pd.DataFrame()  # Empty DataFrame to avoid error\n",
    "\n",
    "initial_eda(product_info, \"Product Information\")\n",
    "initial_eda(reviews, \"Product Reviews\")\n",
    "\n",
    "if isinstance(product_info, pd.DataFrame) and len(product_info) > 0:\n",
    "    print(product_info.describe())\n",
    "else:\n",
    "    print(\"Cannot show describe() - product_info not loaded as DataFrame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Information Data Cleaning\n",
    "product_info_clean = product_info.copy()\n",
    "fill_dict = {\n",
    "    'rating': product_info_clean['rating'].median(),\n",
    "    'reviews': product_info_clean['reviews'].median(),\n",
    "    'size': 'Unknown',\n",
    "    'variation_type': 'None',\n",
    "    'variation_value': 'None',\n",
    "    'variation_desc': 'None',\n",
    "    'ingredients': 'Not Listed',\n",
    "    'highlights': 'None',\n",
    "    'secondary_category': 'Other',\n",
    "    'tertiary_category': 'Other',\n",
    "    'child_max_price': 0,\n",
    "    'child_min_price': 0\n",
    "}\n",
    "product_info_clean = product_info_clean.fillna(fill_dict)\n",
    "product_info_clean['value_price_usd'] = product_info_clean['value_price_usd'].fillna(product_info_clean['price_usd'])\n",
    "product_info_clean['sale_price_usd'] = product_info_clean['sale_price_usd'].fillna(product_info_clean['price_usd'])\n",
    "\n",
    "print(\"Product info cleaned, nulls:\", product_info_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Product Reviews Data Cleaning\n",
    "reviews_clean = reviews.copy()\n",
    "reviews_clean = reviews_clean.drop('Unnamed: 0', axis=1)\n",
    "fill_dict_reviews = {\n",
    "    'is_recommended': 0,\n",
    "    'helpfulness': 0,\n",
    "    'review_text': '',\n",
    "    'review_title': 'No Title',\n",
    "    'skin_tone': 'Not Specified',\n",
    "    'eye_color': 'Not Specified',\n",
    "    'skin_type': 'Not Specified',\n",
    "    'hair_color': 'Not Specified'\n",
    "}\n",
    "reviews_clean = reviews_clean.fillna(fill_dict_reviews)\n",
    "print(\"Reviews cleaned, nulls:\", reviews_clean.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Data Verification and Summary\n",
    "print(\"Product Info columns:\", len(product_info.columns))\n",
    "print(\"Reviews columns:\", len(reviews.columns))\n",
    "\n",
    "# Check columns in the datasets\n",
    "print(\"Product Info columns:\")\n",
    "print(product_info.columns.tolist())\n",
    "print(f\"\\nProduct Info shape: {product_info.shape}\")\n",
    "\n",
    "print(\"\\nReviews columns:\")\n",
    "print(reviews.columns.tolist())\n",
    "print(f\"\\nReviews shape: {reviews.shape}\")\n",
    "\n",
    "print(\"\\nProduct Info Clean columns:\")\n",
    "print(product_info_clean.columns.tolist())\n",
    "\n",
    "print(\"\\nReviews Clean columns:\")\n",
    "print(reviews_clean.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Synthetic Dataset Creation for Fake Review Detection\n",
    "\n",
    "This phase creates synthetic datasets for training a fake review detection model. It merges cleaned reviews with product information, generates fake reviews by shuffling real review texts, combines real and fake data, and splits into training and testing sets with stratification to ensure balanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Synthetic Dataset Creation for Fake Review Detection\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check if synthetic datasets already exist\n",
    "import os\n",
    "if os.path.exists('./synthetic_train.csv') and os.path.exists('./synthetic_test.csv'):\n",
    "    print(\"Synthetic datasets already exist, skipping creation.\")\n",
    "    # Load existing datasets for verification\n",
    "    train_df = pd.read_csv('./synthetic_train.csv')\n",
    "    test_df = pd.read_csv('./synthetic_test.csv')\n",
    "    print(f\"Loaded existing training: {len(train_df)}, testing: {len(test_df)} samples\")\n",
    "else:\n",
    "    print(\"Synthetic datasets not found, proceeding with creation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge reviews with product information\n",
    "print(f\"Reviews clean shape: {reviews_clean.shape}\")\n",
    "print(f\"Product info clean shape: {product_info_clean.shape}\")\n",
    "print(f\"Common product_ids: {len(set(reviews_clean['product_id']).intersection(set(product_info_clean['product_id'])))}\")\n",
    "\n",
    "merged_df = pd.merge(reviews_clean, product_info_clean, on='product_id', how='inner', suffixes=('_review', '_product'))\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(\"Merged columns:\", merged_df.columns.tolist()[:10])  # Show first 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create product_info concatenated column\n",
    "def create_product_info(row):\n",
    "    # Safely handle both suffixed and unsuffixed column names from the merge\n",
    "    def safe_get(r, *keys, default=None):\n",
    "        for k in keys:\n",
    "            if k in r.index and pd.notna(r[k]):\n",
    "                return r[k]\n",
    "        return default\n",
    "\n",
    "    ingredients = safe_get(row, 'ingredients', default='Not Listed')\n",
    "    highlights = safe_get(row, 'highlights', default='None')\n",
    "    brand = safe_get(row, 'brand_name_product', 'brand_name', default='Unknown')\n",
    "    primary_category = safe_get(row, 'primary_category', default='Unknown')\n",
    "    price = safe_get(row, 'price_usd_product', 'price_usd', default=0.0)\n",
    "\n",
    "    try:\n",
    "        price_val = float(price)\n",
    "    except Exception:\n",
    "        price_val = 0.0\n",
    "\n",
    "    return f\"Brand: {brand}, Category: {primary_category}, Price: ${price_val:.2f}, Ingredients: {ingredients}, Highlights: {highlights}\"\n",
    "\n",
    "merged_df['product_info'] = merged_df.apply(create_product_info, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Synthetic Data Generation Function (Safer & Faster)\n",
    "def generate_synthetic_fakes_with_product_swap(merged_df, fake_ratio=0.2, max_rows=None):\n",
    "    \"\"\"\n",
    "    Generate fake reviews by swapping reviews across different product categories while keeping product labels the same.\n",
    "\n",
    "    This safer version:\n",
    "    - Optional `max_rows` to operate on a sampled subset for quick runs.\n",
    "    - Precomputes category groups and all_reviews once.\n",
    "    - Avoids repeated expensive list constructions.\n",
    "    - Adds light progress logging and deterministic randomness.\n",
    "    \"\"\"\n",
    "    import numpy as _np\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    # Work on a sample if requested to avoid very long runs during interactive debugging\n",
    "    if max_rows is not None and len(merged_df) > max_rows:\n",
    "        df = merged_df.sample(n=max_rows, random_state=42).reset_index(drop=True)\n",
    "        print(f\"Using sampled subset of {max_rows} rows for synthetic generation (from {len(merged_df)} total rows)\")\n",
    "    else:\n",
    "        df = merged_df.reset_index(drop=True)\n",
    "\n",
    "    _np.random.seed(42)\n",
    "\n",
    "    # Pre-group by category for efficient sampling\n",
    "    categories = df['primary_category'].unique()\n",
    "    category_groups = {cat: df[df['primary_category'] == cat]['review_text'].tolist() for cat in categories}\n",
    "\n",
    "    # Precompute a flat list of all reviews as fallback\n",
    "    all_reviews = [rev for lst in category_groups.values() for rev in lst]\n",
    "    if not all_reviews:\n",
    "        # If there are no reviews at all, return empty frame\n",
    "        print(\"No reviews available to generate fakes from.\")\n",
    "        df['is_fake'] = 0\n",
    "        return df\n",
    "\n",
    "    num_fake = max(1, int(fake_ratio * len(df)))\n",
    "    fake_indices = _np.random.choice(len(df), num_fake, replace=False)\n",
    "\n",
    "    print(f\"Generating {num_fake} fake reviews...\")\n",
    "\n",
    "    # VECTORIZED APPROACH: Precompute categories for all fake indices\n",
    "    fake_categories = df.loc[fake_indices, 'primary_category'].values\n",
    "    other_categories_list = []\n",
    "    available_reviews_list = []\n",
    "\n",
    "    for cat in tqdm(fake_categories, desc=\"Processing fake categories\"):\n",
    "        other_cats = [c for c in categories if c != cat]\n",
    "        if other_cats:\n",
    "            random_cat = _np.random.choice(other_cats)\n",
    "            reviews = category_groups.get(random_cat, [])\n",
    "            # LIMIT REVIEW LIST SIZE FOR EFFICIENCY - sample at most 1000 reviews per category\n",
    "            if len(reviews) > 1000:\n",
    "                reviews = _np.random.choice(reviews, size=1000, replace=False).tolist()\n",
    "            if reviews:\n",
    "                available_reviews_list.append(reviews)\n",
    "                other_categories_list.append(random_cat)\n",
    "            else:\n",
    "                # Limit fallback list size too\n",
    "                limited_all_reviews = all_reviews if len(all_reviews) <= 1000 else _np.random.choice(all_reviews, size=1000, replace=False).tolist()\n",
    "                available_reviews_list.append(limited_all_reviews)\n",
    "                other_categories_list.append('fallback')\n",
    "        else:\n",
    "            # Limit fallback list size too\n",
    "            limited_all_reviews = all_reviews if len(all_reviews) <= 1000 else _np.random.choice(all_reviews, size=1000, replace=False).tolist()\n",
    "            available_reviews_list.append(limited_all_reviews)\n",
    "            other_categories_list.append('fallback')\n",
    "\n",
    "    # Now generate the fake reviews efficiently - VECTORIZED APPROACH\n",
    "    print(f\"Selecting {len(available_reviews_list)} fake reviews...\")\n",
    "\n",
    "    # VECTORIZED RANDOM SELECTION: Pre-compute all random indices at once for speed\n",
    "    all_reviews_lengths = [len(reviews) for reviews in available_reviews_list]\n",
    "    random_indices = _np.random.randint(0, _np.array(all_reviews_lengths))\n",
    "\n",
    "    new_reviews = []\n",
    "    for i, reviews in enumerate(tqdm(available_reviews_list, desc=\"Selecting fake reviews\")):\n",
    "        if len(reviews) > 0:\n",
    "            new_reviews.append(reviews[random_indices[i]])\n",
    "        else:\n",
    "            # Fallback to all_reviews if category is empty\n",
    "            fallback_idx = _np.random.randint(0, len(all_reviews))\n",
    "            new_reviews.append(all_reviews[fallback_idx])\n",
    "\n",
    "    fake_df = df.iloc[fake_indices].copy()\n",
    "    fake_df['review_text'] = new_reviews\n",
    "    fake_df['is_fake'] = 1\n",
    "\n",
    "    real_df = df.drop(index=fake_indices).copy()\n",
    "    real_df['is_fake'] = 0\n",
    "\n",
    "    # Return combined dataset (shuffled)\n",
    "    combined = pd.concat([real_df, fake_df], ignore_index=True)\n",
    "    combined = combined.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    return combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset with category-aware mismatches\n",
    "# Use max_rows=10000 for faster testing, remove for full dataset\n",
    "combined_df = generate_synthetic_fakes_with_product_swap(merged_df, fake_ratio=0.2, max_rows=10000)\n",
    "\n",
    "print(f\"Real reviews: {len(combined_df[combined_df['is_fake'] == 0])}\")\n",
    "print(f\"Fake reviews: {len(combined_df[combined_df['is_fake'] == 1])}\")\n",
    "print(f\"Total combined: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select required columns and rename\n",
    "name_col = None\n",
    "if 'product_name_product' in combined_df.columns:\n",
    "    name_col = 'product_name_product'\n",
    "elif 'product_name' in combined_df.columns:\n",
    "    name_col = 'product_name'\n",
    "else:\n",
    "    # Fallback: create a product_name column if missing\n",
    "    combined_df['product_name'] = combined_df.get('product_name_product', 'Unknown')\n",
    "    name_col = 'product_name'\n",
    "\n",
    "cols = ['product_id', name_col, 'review_text', 'is_fake', 'product_info']\n",
    "final_df = combined_df[cols].copy()\n",
    "final_df.rename(columns={name_col: 'product_name'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training (80%) and testing (20%) with stratification\n",
    "label_counts = final_df['is_fake'].value_counts()\n",
    "use_stratify = label_counts.min() >= 2\n",
    "if use_stratify:\n",
    "    stratify_col = final_df['is_fake']\n",
    "    print(\"Using stratified split (each class has >=2 samples)\")\n",
    "else:\n",
    "    stratify_col = None\n",
    "    print(\"Dataset too small or imbalanced for stratified split; using random split without stratification\")\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    final_df,\n",
    "    test_size=0.2,\n",
    "    stratify=stratify_col,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(train_df)} samples\")\n",
    "print(f\"Testing set: {len(test_df)} samples\")\n",
    "print(f\"Training fake ratio: {train_df['is_fake'].mean():.3f}\")\n",
    "print(f\"Testing fake ratio: {test_df['is_fake'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save datasets\n",
    "train_df.to_csv('./synthetic_train.csv', index=False)\n",
    "test_df.to_csv('./synthetic_test.csv', index=False)\n",
    "\n",
    "print(\"✓ Synthetic datasets saved as 'synthetic_train.csv' and 'synthetic_test.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verification\n",
    "print(\"\\nSample of training data:\")\n",
    "print(train_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering & RAG Preparation\n",
    "\n",
    "This phase prepares features for the RAG (Retrieval-Augmented Generation) system. It creates comprehensive product profiles, embeds them using Sentence Transformers, stores them in a ChromaDB vector database for efficient retrieval, generates training examples with explanations for fake review detection, and saves them for model fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 2: Feature Engineering & RAG Preparation\n",
    "\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load synthetic datasets\n",
    "train_df = pd.read_csv('./synthetic_train.csv')\n",
    "test_df = pd.read_csv('./synthetic_test.csv')\n",
    "\n",
    "print(f\"Loaded training: {len(train_df)}, testing: {len(test_df)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create product profiles\n",
    "product_profiles = []\n",
    "product_ids = []\n",
    "product_metadatas = []\n",
    "\n",
    "for idx, row in product_info_clean.iterrows():\n",
    "    profile = f\"\"\"\n",
    "Product: {row['product_name']}\n",
    "Brand: {row['brand_name']}\n",
    "Category: {row['primary_category']}\n",
    "Price: ${row['price_usd']:.2f}\n",
    "Ingredients: {row['ingredients'] if pd.notna(row['ingredients']) else 'Not Listed'}\n",
    "Highlights: {row['highlights'] if pd.notna(row['highlights']) else 'None'}\n",
    "\"\"\".strip()\n",
    "    \n",
    "    product_profiles.append(profile)\n",
    "    product_ids.append(str(row['product_id']))\n",
    "    product_metadatas.append({\n",
    "        \"product_name\": row['product_name'],\n",
    "        \"brand_name\": row['brand_name'],\n",
    "        \"category\": row['primary_category'],\n",
    "        \"price\": float(row['price_usd'])\n",
    "    })\n",
    "\n",
    "print(f\"Created {len(product_profiles)} product profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "product_profile_collection = client.get_or_create_collection(\n",
    "    name=\"product_profiles\",\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and store product profiles\n",
    "if product_profile_collection.count() == 0:\n",
    "    # Embed and store product profiles\n",
    "    embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    print(\"Generating embeddings...\")\n",
    "    profile_embeddings = embedding_model.encode(product_profiles, show_progress_bar=True)\n",
    "\n",
    "    batch_size = 5000\n",
    "    print(f\"Storing in batches of {batch_size}...\")\n",
    "    for i in range(0, len(product_profiles), batch_size):\n",
    "        batch_end = min(i + batch_size, len(product_profiles))\n",
    "        batch_ids = product_ids[i:batch_end]\n",
    "        batch_profiles = product_profiles[i:batch_end]\n",
    "        batch_embeddings = profile_embeddings[i:batch_end].tolist()\n",
    "        batch_metadatas = product_metadatas[i:batch_end]\n",
    "\n",
    "        product_profile_collection.add(\n",
    "            ids=batch_ids,\n",
    "            embeddings=batch_embeddings,\n",
    "            documents=batch_profiles,\n",
    "            metadatas=batch_metadatas\n",
    "        )\n",
    "        print(f\"Added {batch_end}/{len(product_profiles)} profiles\")\n",
    "    print(\"Vector database initialized.\")\n",
    "else:\n",
    "    print(\"Vector database already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "test_profile = product_profile_collection.query(\n",
    "    query_texts=[\"A hydrating moisturizer for dry skin\"],\n",
    "    n_results=2\n",
    ")\n",
    "print(\"Test retrieval completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if training examples already exist\n",
    "import os\n",
    "if os.path.exists('./training_examples.json'):\n",
    "    print(\"Training examples already exist, skipping creation.\")\n",
    "    # Load existing for verification\n",
    "    import json\n",
    "    with open('./training_examples.json', 'r') as f:\n",
    "        training_examples = json.load(f)\n",
    "    print(f\"Loaded {len(training_examples)} existing training examples\")\n",
    "else:\n",
    "    print(\"Training examples not found, proceeding with creation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data for examples\n",
    "train_df['review_text'] = train_df['review_text'].fillna('').astype(str)\n",
    "train_df['product_info'] = train_df['product_info'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define heuristic explanation generation function for training examples\n",
    "def generate_heuristic_explanation(row):\n",
    "    # Generate explanation for fake reviews based on simple heuristics\n",
    "    is_fake = int(row.get('is_fake', 0))\n",
    "    if is_fake == 0:\n",
    "        return \"This review matches the product information and appears authentic.\"\n",
    "    else:\n",
    "        review_text = str(row.get('review_text', '')).lower()\n",
    "        product_info = str(row.get('product_info', '')).lower()\n",
    "        explanations = []\n",
    "        if 'skincare' in review_text and 'hair' in product_info:\n",
    "            explanations.append(\"Review discusses skincare but product is for hair.\")\n",
    "        elif 'hair' in review_text and 'skincare' in product_info:\n",
    "            explanations.append(\"Review discusses hair but product is for skincare.\")\n",
    "        if 'natural' in review_text and 'chemical' in product_info:\n",
    "            explanations.append(\"Review praises natural ingredients but product contains chemicals.\")\n",
    "        if 'drying' in review_text and 'hydrating' in product_info:\n",
    "            explanations.append(\"Review mentions drying effects but product is hydrating.\")\n",
    "        elif 'hydrating' in review_text and 'drying' in product_info:\n",
    "            explanations.append(\"Review mentions hydrating effects but product may be drying.\")\n",
    "        if not explanations:\n",
    "            explanations.append(\"Review appears mismatched with product information.\")\n",
    "        return \" \".join(explanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples\n",
    "training_examples = []\n",
    "for idx, row in train_df.iterrows():\n",
    "    # Generate an explanation using the heuristic generator defined earlier.\n",
    "    # Fall back to a short default string if the heuristic fails for any row.\n",
    "    try:\n",
    "        explanation = generate_heuristic_explanation(row)\n",
    "    except Exception as e:\n",
    "        explanation = \"No explanation available (error generating heuristic): \" + str(e)\n",
    "\n",
    "    example = {\n",
    "        \"product_info\": row['product_info'],\n",
    "        \"review_text\": row['review_text'],\n",
    "        \"label\": int(row['is_fake']),\n",
    "        \"explanation_template\": explanation,\n",
    "    }\n",
    "    training_examples.append(example)\n",
    "\n",
    "print(f\"Created {len(training_examples)} training examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training examples\n",
    "import json\n",
    "with open('./training_examples.json', 'w') as f:\n",
    "    json.dump(training_examples, f, indent=2)\n",
    "\n",
    "print(\"Training examples saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample training examples\n",
    "for ex in training_examples[:2]:\n",
    "    print(f\"Label: {ex['label']}, Explanation: {ex['explanation_template'][:50]}...\")\n",
    "\n",
    "print(\"Phase 2 completed: Product profiles in ChromaDB, training examples prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch datasets chromadb sentence-transformers rouge-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: RoBERTa Binary Classification + Dual-LLM Explanations (Optimized)\n",
    "\n",
    "This phase implements a two-stage pipeline for fake review detection with speed optimizations:\n",
    "\n",
    "**Stage 1: RoBERTa Binary Classifier (Optimized)**\n",
    "- Uses RoBERTa-base optimized for classification tasks\n",
    "- Input: \"Product: [brand, category, price] Review: [text]\"\n",
    "- Output: Direct class logits (0=real, 1=fake)\n",
    "- **Speed Optimizations:**\n",
    "  - Batch size: 64 (up from 16)\n",
    "  - Mixed precision: fp16=True\n",
    "  - Gradient checkpointing enabled\n",
    "  - Pre-tokenized dataset caching\n",
    "  - Parallel data loading (num_workers=4)\n",
    "  - Reduced max_length=256 (from 512)\n",
    "  - RAG queries removed from training loop\n",
    "\n",
    "**Stage 2: GPT-2 Explanation Generation**\n",
    "- Separate LLM for generating explanations post-classification\n",
    "- Uses RAG context from ChromaDB for fake reviews\n",
    "- Prompt: \"Review: [text] Product: [info] Context: [similar products] Explain why this review might be fake:\"\n",
    "- Benefits: Isolated explanation task, better quality control\n",
    "\n",
    "**Key Improvements over T5:**\n",
    "- ✓ Direct logit predictions → accurate metrics\n",
    "- ✓ Training time: ~30-45 min (down from 3+ hours)\n",
    "- ✓ Production-ready binary classification\n",
    "- ✓ Separate explanation generation for better control\n",
    "- ✓ Memory efficient with gradient checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 3: RoBERTa Binary Classification + Dual-LLM Explanations (Optimized)\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from datasets import Dataset, load_from_disk, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import chromadb\n",
    "import os\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"GPU cache cleared\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONSOLIDATED FUNCTION DEFINITIONS FOR PHASE 3 (refactored)\n",
    "# - Extracted small helpers to reduce duplication\n",
    "# - Clear docstrings and return contracts added\n",
    "# =============================================================================\n",
    "\n",
    "class RobertaDatasetFormatter:\n",
    "    \"\"\"Format inputs for RoBERTa classification.\n",
    "\n",
    "    Methods\n",
    "    - format_input(product_info, review_text) -> str\n",
    "    - tokenize_function(examples) -> dict\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, max_input_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "\n",
    "    def format_input(self, product_info, review_text):\n",
    "        \"\"\"Return a human-readable input string for the model.\"\"\"\n",
    "        input_text = f\"Product information: {product_info}\\nReview: {review_text}\\n\\nIs this review authentic for this product?\"\n",
    "        return input_text\n",
    "\n",
    "    def tokenize_function(self, examples):\n",
    "        inputs = examples[\"input_text\"]\n",
    "        labels = examples[\"label\"]\n",
    "        model_inputs = self.tokenizer(\n",
    "            inputs,\n",
    "            max_length=self.max_input_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "        )\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        return model_inputs\n",
    "\n",
    "\n",
    "def prepare_training_data_roberta(examples, formatter):\n",
    "    \"\"\"Prepare lists of input_text and labels from training examples.\n",
    "\n",
    "    Returns\n",
    "    - inputs: list[str]\n",
    "    - labels: list[int]\n",
    "    \"\"\"\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    print(\"Preparing training data for RoBERTa...\")\n",
    "    for i, ex in enumerate(examples):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"  Processed {i}/{len(examples)} examples\")\n",
    "        input_text = formatter.format_input(ex['product_info'], ex['review_text'])\n",
    "        inputs.append(input_text)\n",
    "        labels.append(int(ex['label']))\n",
    "    return inputs, labels\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute standard classification metrics given (predictions, labels).\n",
    "\n",
    "    Expects eval_pred: (logits, labels)\n",
    "    Returns a dict of floats suitable for HF Trainer logging.\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, zero_division=0)\n",
    "    recall = recall_score(labels, preds, zero_division=0)\n",
    "    f1 = f1_score(labels, preds, zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, torch.softmax(torch.tensor(logits), dim=1)[:, 1].numpy())\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"auc\": float(auc) if auc is not None else None,\n",
    "    }\n",
    "\n",
    "# ---------- Inference helper to avoid duplicated code ----------\n",
    "\n",
    "def _run_model_inference(tokenizer, model, input_texts, device, max_length=256):\n",
    "    \"\"\"Tokenize inputs, run the model, and return (pred_labels, confidences, logits).\n",
    "\n",
    "    - input_texts: list[str]\n",
    "    - returns: (preds: np.ndarray, confidences: np.ndarray, logits: np.ndarray)\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(input_texts, return_tensors=\"pt\", max_length=max_length, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits.detach().cpu().numpy()\n",
    "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "        preds = np.argmax(logits, axis=1)\n",
    "        confidences = probs[np.arange(len(preds)), preds]\n",
    "    return preds, confidences, logits\n",
    "\n",
    "# ---------- Classification wrappers (single-instance and batch) ----------\n",
    "\n",
    "def classify_review(product_info, review_text, tokenizer, model, formatter, device):\n",
    "    \"\"\"Classify a single review and return ('FAKE'|'REAL', confidence)\n",
    "\n",
    "    Uses the shared inference helper for consistency and easier testing.\n",
    "    \"\"\"\n",
    "    input_text = formatter.format_input(product_info, review_text)\n",
    "    preds, confs, _ = _run_model_inference(tokenizer, model, [input_text], device)\n",
    "    pred = int(preds[0])\n",
    "    confidence = float(confs[0])\n",
    "    return (\"FAKE\" if pred == 1 else \"REAL\"), confidence\n",
    "\n",
    "\n",
    "def classify_review_roberta_batch(product_infos, review_texts, tokenizer, model, formatter, device):\n",
    "    \"\"\"Batch classify multiple reviews and return (pred_labels, confidences).\n",
    "\n",
    "    Inputs are parallel lists of product_infos and review_texts.\n",
    "    \"\"\"\n",
    "    input_texts = [formatter.format_input(prod, rev) for prod, rev in zip(product_infos, review_texts)]\n",
    "    preds, confidences, logits = _run_model_inference(tokenizer, model, input_texts, device)\n",
    "    return preds.tolist(), confidences.tolist()\n",
    "\n",
    "# ---------- RAG and explanation generation helpers (kept as before) ----------\n",
    "\n",
    "def get_rag_context(review_text, product_info=\"\"):\n",
    "    \"\"\"Get RAG context for a review by querying ChromaDB\"\"\"\n",
    "    try:\n",
    "        results = product_profile_collection.query(\n",
    "            query_texts=[review_text],\n",
    "            n_results=3\n",
    "        )\n",
    "        contexts = results['documents'][0] if results.get('documents') else []\n",
    "        return \" \".join(contexts[:2])\n",
    "    except Exception as e:\n",
    "        return f\"Error retrieving context: {str(e)}\"\n",
    "\n",
    "\n",
    "def generate_explanation_with_gpt2(product_info, review_text, prediction, rag_context):\n",
    "    prompt = f\"Product: {product_info}\\nReview: {review_text}\\nContext: {rag_context}\\n\\nExplain why this review might be {'fake' if prediction == 1 else 'real'}:\"\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs['input_ids'].shape[1] + 100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "        )\n",
    "    explanation = gpt2_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return explanation.strip() if explanation.strip() else \"Unable to generate explanation.\"\n",
    "\n",
    "\n",
    "def generate_explanation(review_text, prediction, confidence):\n",
    "    prompt = f\"Review: {review_text}\\nPrediction: {'Real' if prediction == 0 else 'Fake'}\\nConfidence: {confidence:.3f}\\n\\nExplain why this review is predicted as {'real' if prediction == 0 else 'fake'}:\"\n",
    "    inputs = gpt2_tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = gpt2_model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs['input_ids'].shape[1] + 100,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=gpt2_tokenizer.eos_token_id,\n",
    "        )\n",
    "    explanation = gpt2_tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "    return explanation.strip()\n",
    "\n",
    "\n",
    "def explain_prediction(review_text, prediction, confidence):\n",
    "    try:\n",
    "        explanation = generate_explanation(review_text, prediction, confidence)\n",
    "        return explanation if explanation else \"Unable to generate explanation.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error generating explanation: {str(e)}\"\n",
    "\n",
    "# Batch processing helpers and main evaluation orchestration remain unchanged in contract\n",
    "\n",
    "def setup_batch_evaluation(test_df, batch_size=32):\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    explanations = []\n",
    "    confidences = []\n",
    "    print(f\"Setting up batch evaluation with batch size {batch_size}\")\n",
    "    print(f\"Total test samples: {len(test_df)}\")\n",
    "    return predictions, ground_truth, explanations, confidences, batch_size\n",
    "\n",
    "\n",
    "def get_rag_contexts_for_fake_reviews(fake_reviews):\n",
    "    batch_rag_contexts = []\n",
    "    for review_text in fake_reviews:\n",
    "        rag_context = get_rag_context(review_text, \"\")\n",
    "        batch_rag_contexts.append(rag_context)\n",
    "    return batch_rag_contexts\n",
    "\n",
    "\n",
    "def generate_explanations_for_fake_reviews(batch_df, fake_indices, batch_rag_contexts):\n",
    "    fake_explanations = []\n",
    "    for i, idx in enumerate(fake_indices):\n",
    "        row = batch_df.iloc[idx]\n",
    "        explanation = generate_explanation_with_gpt2(\n",
    "            row['product_info'],\n",
    "            row['review_text'],\n",
    "            1,\n",
    "            batch_rag_contexts[i],\n",
    "        )\n",
    "        fake_explanations.append(explanation)\n",
    "    return fake_explanations\n",
    "\n",
    "\n",
    "def combine_batch_explanations(batch_df, batch_predictions, fake_indices, fake_explanations):\n",
    "    batch_explanations = []\n",
    "    explanation_idx = 0\n",
    "    for i in range(len(batch_df)):\n",
    "        if i in fake_indices:\n",
    "            batch_explanations.append(fake_explanations[explanation_idx])\n",
    "            explanation_idx += 1\n",
    "        else:\n",
    "            batch_explanations.append(\"This review appears authentic and matches the product information.\")\n",
    "    return batch_explanations\n",
    "\n",
    "\n",
    "def evaluate_model(test_df, model, tokenizer, chromadb_client, formatter, batch_size=32):\n",
    "    predictions, ground_truth, explanations, confidences, _ = setup_batch_evaluation(test_df, batch_size)\n",
    "    print(\"Starting batch evaluation...\")\n",
    "    for start_idx in tqdm(range(0, len(test_df), batch_size), desc=\"Processing batches\"):\n",
    "        end_idx = min(start_idx + batch_size, len(test_df))\n",
    "        batch_df = test_df.iloc[start_idx:end_idx]\n",
    "        batch_product_infos = batch_df['product_info'].tolist() if 'product_info' in batch_df.columns else [''] * len(batch_df)\n",
    "        batch_review_texts = batch_df['review_text'].tolist()\n",
    "        batch_preds, batch_confs = classify_review_roberta_batch(batch_product_infos, batch_review_texts, tokenizer, model, formatter, device)\n",
    "        fake_indices = [i for i, pred in enumerate(batch_preds) if pred == 1]\n",
    "        if fake_indices:\n",
    "            fake_reviews = batch_df.iloc[fake_indices]['review_text'].tolist()\n",
    "            batch_rag_contexts = get_rag_contexts_for_fake_reviews(fake_reviews)\n",
    "            fake_explanations = generate_explanations_for_fake_reviews(batch_df, fake_indices, batch_rag_contexts)\n",
    "            batch_explanations = combine_batch_explanations(batch_df, batch_preds, fake_indices, fake_explanations)\n",
    "        else:\n",
    "            batch_explanations = [\"This review appears authentic and matches the product information.\"] * len(batch_df)\n",
    "        predictions.extend(batch_preds)\n",
    "        confidences.extend(batch_confs)\n",
    "        explanations.extend(batch_explanations)\n",
    "        ground_truth.extend(batch_df['is_fake'].astype(int).tolist())\n",
    "    return predictions, ground_truth, explanations, confidences\n",
    "\n",
    "# End of consolidated/refactored cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced compute_metrics that includes per-class precision/recall and confusion matrix\n",
    "# This will help monitor whether both classes are being predicted during training\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision = precision_score(labels, preds, zero_division=0)\n",
    "    recall = recall_score(labels, preds, zero_division=0)\n",
    "    f1 = f1_score(labels, preds, zero_division=0)\n",
    "\n",
    "    # Per-class metrics\n",
    "    try:\n",
    "        precision_per_class = precision_score(labels, preds, average=None, zero_division=0)\n",
    "        recall_per_class = recall_score(labels, preds, average=None, zero_division=0)\n",
    "        prec_c0 = float(precision_per_class[0]) if len(precision_per_class) > 0 else 0.0\n",
    "        prec_c1 = float(precision_per_class[1]) if len(precision_per_class) > 1 else 0.0\n",
    "        rec_c0 = float(recall_per_class[0]) if len(recall_per_class) > 0 else 0.0\n",
    "        rec_c1 = float(recall_per_class[1]) if len(recall_per_class) > 1 else 0.0\n",
    "    except Exception:\n",
    "        prec_c0 = prec_c1 = rec_c0 = rec_c1 = 0.0\n",
    "\n",
    "    # Confusion matrix as JSON-serializable list\n",
    "    try:\n",
    "        cm = confusion_matrix(labels, preds).tolist()\n",
    "    except Exception:\n",
    "        cm = None\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy),\n",
    "        \"precision\": float(precision),\n",
    "        \"recall\": float(recall),\n",
    "        \"f1\": float(f1),\n",
    "        \"precision_class_0\": prec_c0,\n",
    "        \"precision_class_1\": prec_c1,\n",
    "        \"recall_class_0\": rec_c0,\n",
    "        \"recall_class_1\": rec_c1,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading and preparation\n",
    "with open('./training_examples.json', 'r') as f:\n",
    "    training_examples = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(training_examples)} training examples\")\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./chroma_data\")\n",
    "product_profile_collection = client.get_collection(name=\"product_profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check training data labels\n",
    "print(\"Sample training examples:\")\n",
    "for i, ex in enumerate(training_examples[:5]):\n",
    "    print(f\"Example {i}: Label={ex['label']}, Product={ex['product_info'][:100]}..., Review={ex['review_text'][:100]}...\")\n",
    "\n",
    "# Check label distribution\n",
    "labels = [ex['label'] for ex in training_examples]\n",
    "print(f\"\\nLabel distribution: {np.bincount(labels)} (0=real, 1=fake)\")\n",
    "print(f\"Label ratio: {np.mean(labels):.3f} fake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Create a small balanced debug dataset if possible\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DEBUG: Creating small balanced dataset\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Sample small balanced dataset\n",
    "debug_size = 1000  # Target size\n",
    "real_examples = [ex for ex in training_examples if ex['label'] == 0]\n",
    "fake_examples = [ex for ex in training_examples if ex['label'] == 1]\n",
    "\n",
    "# Determine feasible balanced size\n",
    "min_class = min(len(real_examples), len(fake_examples))\n",
    "if min_class == 0:\n",
    "    print(\"Not enough examples to create a balanced debug dataset; skipping debug sampling.\")\n",
    "else:\n",
    "    actual_half = min(min_class, debug_size // 2)\n",
    "    np.random.seed(42)\n",
    "    debug_real = np.random.choice(real_examples, size=actual_half, replace=False).tolist()\n",
    "    debug_fake = np.random.choice(fake_examples, size=actual_half, replace=False).tolist()\n",
    "\n",
    "    debug_training_examples = debug_real + debug_fake\n",
    "    np.random.shuffle(debug_training_examples)\n",
    "\n",
    "    print(f\"Debug dataset: {len(debug_training_examples)} examples ({actual_half} real, {actual_half} fake)\")\n",
    "\n",
    "    # COMMENTED OUT: Override training_examples for debugging\n",
    "    # training_examples = debug_training_examples\n",
    "\n",
    "print(\"Using FULL training dataset for production training...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check what the formatted inputs look like\n",
    "print(\"\\nSample formatted inputs:\")\n",
    "\n",
    "# Ensure `tokenizer` is available. If it's missing, try to load a saved tokenizer from\n",
    "# the local `./fake_review_detector_roberta` directory (present in the repo),\n",
    "# otherwise fall back to the public 'roberta-base' tokenizer.\n",
    "try:\n",
    "    tokenizer\n",
    "except NameError:\n",
    "    try:\n",
    "        from transformers import RobertaTokenizer\n",
    "    except Exception as e:\n",
    "        raise ImportError(\"transformers not available: \" + str(e))\n",
    "    import os\n",
    "    model_dir = os.path.join('.', 'fake_review_detector_roberta')\n",
    "    if os.path.isdir(model_dir):\n",
    "        tokenizer = RobertaTokenizer.from_pretrained(model_dir)\n",
    "        print(f\"Loaded tokenizer from {model_dir}\")\n",
    "    else:\n",
    "        tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        print(\"Loaded tokenizer 'roberta-base' as fallback\")\n",
    "\n",
    "# Instantiate the formatter (assumes the class RobertaDatasetFormatter is defined earlier in the notebook)\n",
    "try:\n",
    "    formatter = RobertaDatasetFormatter(tokenizer)\n",
    "except NameError:\n",
    "    raise NameError(\"RobertaDatasetFormatter is not defined. Please run the cell that defines this class before running this debug cell.\")\n",
    "\n",
    "# Show a few formatted examples and tokenization result\n",
    "for i, ex in enumerate(training_examples[:3]):\n",
    "    product_info = ex.get('product_info', '')\n",
    "    review_text = ex.get('review_text', '')\n",
    "    input_text = formatter.format_input(product_info, review_text)\n",
    "    print(f\"--- Example {i} ---\")\n",
    "    print(input_text)\n",
    "    tokens = tokenizer.tokenize(input_text)\n",
    "    print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and tokenizer setup - Using RoBERTa-base for classification\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to device: {next(model.parameters()).device}\")\n",
    "\n",
    "formatter = RobertaDatasetFormatter(tokenizer)\n",
    "\n",
    "print(f\"\\nLoaded model: {model_name}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if tokenized dataset already exists\n",
    "force_regenerate = True\n",
    "\n",
    "tokenized_dataset_path = \"./tokenized_roberta_dataset\"\n",
    "if os.path.exists(tokenized_dataset_path) and not force_regenerate:\n",
    "    print(\"Loading pre-tokenized dataset...\")\n",
    "    tokenized_datasets = load_from_disk(tokenized_dataset_path)\n",
    "    train_dataset = tokenized_datasets[\"train\"]\n",
    "    val_dataset = tokenized_datasets[\"validation\"]\n",
    "else:\n",
    "    # Prepare training data\n",
    "    train_inputs, train_labels = prepare_training_data_roberta(training_examples, formatter)\n",
    "\n",
    "    train_dataset = Dataset.from_dict({\n",
    "        \"input_text\": train_inputs,\n",
    "        \"label\": train_labels\n",
    "    })\n",
    "\n",
    "    print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "\n",
    "    # Train/validation split\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Creating Train/Validation Split\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # Determine whether stratification is feasible\n",
    "    try:\n",
    "        label_counts = np.bincount(train_labels)\n",
    "        use_stratify = label_counts.min() >= 2\n",
    "    except Exception:\n",
    "        use_stratify = False\n",
    "\n",
    "    if use_stratify:\n",
    "        stratify_labels = train_labels\n",
    "        print(\"Using stratified split for tokenization precompute\")\n",
    "    else:\n",
    "        stratify_labels = None\n",
    "        print(\"Not enough examples for stratified split; using random split\")\n",
    "\n",
    "    train_split_indices, val_split_indices = train_test_split(\n",
    "        range(len(train_dataset)),\n",
    "        test_size=0.2,\n",
    "        stratify=stratify_labels,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    train_split = train_dataset.select(train_split_indices)\n",
    "    val_split = train_dataset.select(val_split_indices)\n",
    "\n",
    "    print(f\"Training split size: {len(train_split)}\")\n",
    "    print(f\"Validation split size: {len(val_split)}\")\n",
    "\n",
    "    # Tokenize datasets (precompute for speed)\n",
    "    print(\"\\nTokenizing datasets (this may take a moment)...\")\n",
    "    tokenized_train_dataset = train_split.map(formatter.tokenize_function, batched=True, num_proc=1)\n",
    "    tokenized_val_dataset = val_split.map(formatter.tokenize_function, batched=True, num_proc=1)\n",
    "\n",
    "    # Save tokenized datasets\n",
    "    tokenized_datasets = DatasetDict({\n",
    "        \"train\": tokenized_train_dataset,\n",
    "        \"validation\": tokenized_val_dataset\n",
    "    })\n",
    "    tokenized_datasets.save_to_disk(tokenized_dataset_path)\n",
    "    print(f\"Tokenized datasets saved to {tokenized_dataset_path}\")\n",
    "\n",
    "    train_dataset = tokenized_train_dataset\n",
    "    val_dataset = tokenized_val_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments and trainer setup (optimized for speed)\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Determine batch sizes depending on device\n",
    "if device.type == \"cpu\":\n",
    "    train_batch_size = 4\n",
    "    eval_batch_size = 4\n",
    "    grad_accum_steps = 4\n",
    "else:\n",
    "    train_batch_size = 64  # Increased from 16\n",
    "    eval_batch_size = 64   # Increased from 16\n",
    "    grad_accum_steps = 1   # Reduced from 2\n",
    "\n",
    "# Compute class weights from the raw training labels (train_labels is defined earlier when preparing inputs)\n",
    "try:\n",
    "    unique_classes = np.unique(train_labels)\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=np.array(train_labels))\n",
    "    class_weights_dict = {int(c): float(w) for c, w in zip([0, 1], class_weights)}\n",
    "    print(f\"Computed class weights: {class_weights_dict}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: could not compute class weights (falling back to 1.0): {e}\")\n",
    "    class_weights = np.array([1.0, 1.0])\n",
    "\n",
    "# Move class weights to device for loss calculation\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
    "\n",
    "# Use weighted loss via a custom Trainer to avoid modifying model code\n",
    "from transformers import Trainer\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        \"\"\"Custom compute_loss that accepts extra kwargs from HF Trainer.\n",
    "\n",
    "        This avoids TypeError when the Trainer passes framework-specific keywords\n",
    "        such as `num_items_in_batch`.\n",
    "        \"\"\"\n",
    "        # Trainer already moves tensors to device; labels are in inputs['labels']\n",
    "        labels = inputs.get(\"labels\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        # If model returned a precomputed loss (some models do), prefer that when labels are missing\n",
    "        logits = getattr(outputs, \"logits\", None)\n",
    "\n",
    "        if labels is None:\n",
    "            # If model provided a loss (e.g., when labels are embedded), use it; otherwise fallback to 0\n",
    "            loss = getattr(outputs, \"loss\", None)\n",
    "            if loss is None:\n",
    "                # Fallback: zero tensor on same device as model\n",
    "                loss = torch.tensor(0.0, device=next(model.parameters()).device)\n",
    "        else:\n",
    "            # Use CrossEntropyLoss with class weights for multi-class/binary classification\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "            # Ensure logits exist\n",
    "            if logits is None:\n",
    "                outputs = model(**inputs)\n",
    "                logits = outputs.logits\n",
    "            loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# TrainingArguments - prefer accuracy or a composite metric for early stopping when F1 may be zero early\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./fake_review_detector_roberta\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=train_batch_size,\n",
    "    per_device_eval_batch_size=eval_batch_size,\n",
    "    gradient_accumulation_steps=grad_accum_steps,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_steps=1,  # Changed from 50 to 1 to see training loss\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    seed=42,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # Use accuracy (or 'f1' once F1 becomes meaningful)\n",
    "    greater_is_better=True,\n",
    "    # Speed optimizations\n",
    "    fp16=True,  # Mixed precision training\n",
    "    dataloader_num_workers=4,  # Parallel data loading\n",
    "    dataloader_pin_memory=True,  # Faster GPU transfer\n",
    ")\n",
    "\n",
    "# Instantiate our WeightedTrainer instead of the default Trainer\n",
    "# Use a plain EarlyStoppingCallback() without keyword args to avoid mismatches across transformer versions\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized with weighted loss and metric_for_best_model=\\\"accuracy\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check dataset sizes\n",
    "print(\"Dataset sizes:\")\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Validation dataset: {len(val_dataset)} samples\")\n",
    "print(f\"Test dataframe: {len(test_df)} samples\")\n",
    "\n",
    "# Check label distributions\n",
    "train_labels = [ex['label'] for ex in train_dataset]\n",
    "val_labels = [ex['label'] for ex in val_dataset]\n",
    "test_labels = test_df['is_fake'].astype(int).tolist()\n",
    "\n",
    "print(f\"\\nTrain labels distribution: {np.bincount(train_labels)} (0=real, 1=fake)\")\n",
    "print(f\"Val labels distribution: {np.bincount(val_labels)} (0=real, 1=fake)\")\n",
    "print(f\"Test labels distribution: {np.bincount(test_labels)} (0=real, 1=fake)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data quality: duplicates and label leakage\n",
    "print(\"\\nData Quality Checks:\")\n",
    "\n",
    "# Check for duplicates in training data (using original training_examples)\n",
    "train_texts = [ex['review_text'] for ex in training_examples[:len(train_dataset)]]  # Match the split size\n",
    "val_texts = [ex['review_text'] for ex in training_examples[len(train_dataset):len(train_dataset)+len(val_dataset)]]  # Val split\n",
    "test_texts = test_df['review_text'].tolist()\n",
    "\n",
    "print(f\"Train duplicates: {len(train_texts) - len(set(train_texts))}\")\n",
    "print(f\"Val duplicates: {len(val_texts) - len(set(val_texts))}\")\n",
    "print(f\"Test duplicates: {len(test_texts) - len(set(test_texts))}\")\n",
    "\n",
    "# Check for overlap between splits\n",
    "train_set = set(train_texts)\n",
    "val_set = set(val_texts)\n",
    "test_set = set(test_texts)\n",
    "\n",
    "train_val_overlap = len(train_set.intersection(val_set))\n",
    "train_test_overlap = len(train_set.intersection(test_set))\n",
    "val_test_overlap = len(val_set.intersection(test_set))\n",
    "\n",
    "print(f\"Train-Val overlap: {train_val_overlap}\")\n",
    "print(f\"Train-Test overlap: {train_test_overlap}\")\n",
    "print(f\"Val-Test overlap: {val_test_overlap}\")\n",
    "\n",
    "# Check label balance\n",
    "print(f\"\\nLabel balance check:\")\n",
    "print(f\"Train fake ratio: {np.mean([ex['label'] for ex in train_dataset]):.3f}\")\n",
    "print(f\"Val fake ratio: {np.mean([ex['label'] for ex in val_dataset]):.3f}\")\n",
    "print(f\"Test fake ratio: {test_df['is_fake'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "import traceback\n",
    "print(\"\\nStarting RoBERTa training (optimized)...\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    best_metric = getattr(trainer.state, 'best_metric', None)\n",
    "    if best_metric is not None:\n",
    "        try:\n",
    "            print(f\"Training completed! Best metric: {best_metric:.4f}\")\n",
    "        except Exception:\n",
    "            print(\"Training completed! Best metric:\", best_metric)\n",
    "    else:\n",
    "        print(\"Training completed! No best_metric available in trainer.state.\")\n",
    "except Exception as e:\n",
    "    print(\"Training failed with exception:\")\n",
    "    traceback.print_exc()\n",
    "    # Re-raise to surface the error to the notebook if desired\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "trainer.save_model(\"./fake_review_detector_roberta\")\n",
    "tokenizer.save_pretrained(\"./fake_review_detector_roberta\")\n",
    "print(\"RoBERTa model saved successfully to './fake_review_detector_roberta'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: RAG Pipeline Integration\n",
    "\n",
    "This phase integrates the RAG (Retrieval-Augmented Generation) pipeline to enhance explainability for fake review detection. Now using RoBERTa for classification and GPT-2 for explanation generation.\n",
    "\n",
    "**Objective:** Enable explainability for fake reviews with dual-LLM approach\n",
    "\n",
    "**Tasks:**\n",
    "- Use RoBERTa for binary classification (0=real, 1=fake)\n",
    "- For fake reviews: Query ChromaDB for semantically similar products\n",
    "- Pass to GPT-2: \"Review: [text] Product: [info] Context: [similar products] Explain why this review might be fake:\"\n",
    "- Generate contextual explanations for mismatches\n",
    "\n",
    "**Dual-LLM Benefits:**\n",
    "- RoBERTa: Optimized for classification, direct logit outputs\n",
    "- GPT-2: Specialized for text generation and explanations\n",
    "- RAG: Provides product context for better mismatch detection\n",
    "- Isolated tasks: Better performance and debugging\n",
    "\n",
    "**Evaluation metrics:**\n",
    "- Classification accuracy, precision, recall, F1-score\n",
    "- ROUGE/BLEU scores for explanation quality\n",
    "- Human evaluation of explanation coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get_rag_context function moved to consolidated definitions cell (Phase 3 imports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 4 Summary\n",
    "\n",
    "**Objective:** Implement end-to-end evaluation pipeline with RAG-augmented explanations\n",
    "\n",
    "**Key Components:**\n",
    "- **Batch Processing:** Efficient evaluation with configurable batch sizes (32)\n",
    "- **RAG Integration:** Context retrieval for explanation generation using ChromaDB\n",
    "- **GPT-2 Explanations:** Natural language explanations for detected fake reviews\n",
    "- **Modular Design:** Separate functions for classification, context retrieval, and explanation generation\n",
    "\n",
    "**Functions Created:**\n",
    "- `setup_batch_evaluation()`: Initialize evaluation data structures\n",
    "- `batch_classify_reviews()`: Classify reviews in batches\n",
    "- `get_rag_contexts_for_fake_reviews()`: Retrieve RAG contexts for fake reviews\n",
    "- `generate_explanations_for_fake_reviews()`: Generate GPT-2 explanations\n",
    "- `combine_batch_explanations()`: Merge explanations for real and fake reviews\n",
    "- `evaluate_model()`: Main orchestration function\n",
    "\n",
    "**Performance Optimizations:**\n",
    "- Batch processing reduces memory usage and improves speed\n",
    "- Selective explanation generation (only for fake reviews)\n",
    "- Efficient data structures for result collection\n",
    "\n",
    "**Next Steps:** Phase 5 will focus on RoBERTa model evaluation and benchmarking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 5: Testing & Validation\n",
    "\n",
    "**Objective:** Evaluate end-to-end performance\n",
    "\n",
    "**Tasks:**\n",
    "- Run inference on test set - Get classifications and explanations\n",
    "- Analyze failure cases - Where does the model misclassify or provide poor explanations?\n",
    "- Benchmark against baselines - Compare to simple text similarity baselines (cosine similarity, TF-IDF, bag of words)\n",
    "- Generate evaluation report - Document accuracy, explanation quality, computational costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "roberta_predictions = []\n",
    "roberta_confidences = []\n",
    "\n",
    "# Set batch size for evaluation\n",
    "batch_size = 64\n",
    "print(f\"Starting RoBERTa evaluation with batch size {batch_size}\")\n",
    "print(f\"Total test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute batch evaluation\n",
    "print(\"\\n[1] RoBERTa Binary Classification - Predictions\")\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 64  # Define batch size\n",
    "\n",
    "for start_idx in tqdm(range(0, len(test_df), batch_size), desc=\"Processing RoBERTa batches\"):\n",
    "    end_idx = min(start_idx + batch_size, len(test_df))\n",
    "    batch_product_infos = test_df.iloc[start_idx:end_idx]['product_info'].tolist()\n",
    "    batch_review_texts = test_df.iloc[start_idx:end_idx]['review_text'].tolist()\n",
    "\n",
    "    # Pass tokenizer, model, formatter, device per refactor\n",
    "    batch_preds, batch_confs = classify_review_roberta_batch(batch_product_infos, batch_review_texts, tokenizer, model, formatter, device)\n",
    "    roberta_predictions.extend(batch_preds)\n",
    "    roberta_confidences.extend(batch_confs)\n",
    "\n",
    "print(f\"Completed RoBERTa evaluation: {len(roberta_predictions)} predictions generated\")\n",
    "\n",
    "# Calculate metrics for RoBERTa\n",
    "ground_truth = test_df['is_fake'].astype(int).tolist()\n",
    "roberta_metrics = calculate_metrics(np.array(roberta_predictions), np.array(ground_truth), np.array(roberta_confidences))\n",
    "\n",
    "print(\"\\nRoBERTa Evaluation Metrics:\")\n",
    "for metric, value in roberta_metrics.items():\n",
    "    if value is not None:\n",
    "        print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric.capitalize()}: N/A\")\n",
    "\n",
    "# Debug: Check distributions\n",
    "print(f\"\\nPredictions distribution: {np.bincount(roberta_predictions)} (0=real, 1=fake)\")\n",
    "print(f\"Ground truth distribution: {np.bincount(ground_truth)} (0=real, 1=fake)\")\n",
    "print(f\"Sample predictions: {roberta_predictions[:10]}\")\n",
    "print(f\"Sample ground truth: {ground_truth[:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calculate_metrics and print evaluation summary (added to resolve NameError from earlier cell)\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def calculate_metrics(predictions, labels, confidences):\n",
    "    \"\"\"Calculate evaluation metrics for predictions.\n",
    "\n",
    "    predictions, labels, confidences are numpy arrays or lists.\n",
    "    Returns dict with accuracy, precision, recall, f1_score, auc (or None).\n",
    "    \"\"\"\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(labels, predictions, average='weighted', zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(labels, confidences)\n",
    "    except Exception:\n",
    "        auc = None\n",
    "    return {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'auc': float(auc) if auc is not None else None,\n",
    "    }\n",
    "\n",
    "# Compute and print metrics using variables already present in kernel\n",
    "try:\n",
    "    roberta_metrics = calculate_metrics(np.array(roberta_predictions), np.array(ground_truth), np.array(roberta_confidences))\n",
    "    print(\"\\nRoBERTa Evaluation Metrics:\")\n",
    "    for metric, value in roberta_metrics.items():\n",
    "        if value is not None:\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "        else:\n",
    "            print(f\"{metric.capitalize()}: N/A\")\n",
    "    print(f\"\\nPredictions distribution: {np.bincount(roberta_predictions)} (0=real, 1=fake)\")\n",
    "    print(f\"Ground truth distribution: {np.bincount(ground_truth)} (0=real, 1=fake)\")\n",
    "except Exception as e:\n",
    "    print('Error computing/printing metrics:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 5 Summary\n",
    "\n",
    "**Objective:** Evaluate RoBERTa model performance on test data\n",
    "\n",
    "**Key Components:**\n",
    "- **Batch Inference:** Optimized RoBERTa classification with batch processing\n",
    "- **Confidence Scores:** Probability-based confidence metrics for predictions\n",
    "- **Performance Tracking:** Progress monitoring with tqdm for large datasets\n",
    "\n",
    "**Functions Created:**\n",
    "- `classify_review_roberta_batch()`: Batch classification using RoBERTa model\n",
    "- Batch processing loop with configurable batch size (64)\n",
    "\n",
    "**Technical Details:**\n",
    "- **Input Formatting:** Uses `RobertaDatasetFormatter` for consistent input formatting\n",
    "- **GPU Acceleration:** Leverages CUDA device for fast inference\n",
    "- **Memory Efficient:** Processes data in batches to manage memory usage\n",
    "- **Output:** Predictions (0=real, 1=fake) and confidence scores\n",
    "\n",
    "**Results:**\n",
    "- `roberta_predictions`: Binary classification results\n",
    "- `roberta_confidences`: Confidence scores for each prediction\n",
    "- Ready for comparison with ground truth labels\n",
    "\n",
    "**Next Steps:** Compare RoBERTa performance against baseline methods and analyze results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics cell: run multiple leakage and training checks (overlap, label dist, preds, chroma, correlations)\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print('Starting diagnostics checks...')\n",
    "\n",
    "# helper\n",
    "def hash_series_texts(s):\n",
    "    return s.dropna().astype(str).map(lambda x: hashlib.md5(x.strip().encode('utf-8')).hexdigest())\n",
    "\n",
    "results = {}\n",
    "\n",
    "# 1) Overlap checks using pandas DataFrames if available\n",
    "try:\n",
    "    if 'train_df' in globals() and 'val_df' in globals():\n",
    "        h_train = set(hash_series_texts(train_df['review_text']))\n",
    "        h_val = set(hash_series_texts(val_df['review_text']))\n",
    "        txt_overlap = len(h_train & h_val)\n",
    "        results['train_val_text_overlap'] = txt_overlap\n",
    "        print('train/val exact text overlap:', txt_overlap)\n",
    "    else:\n",
    "        print('train_df or val_df not in globals(); skipping pandas text overlap check')\n",
    "except Exception as e:\n",
    "    print('Error computing pandas train/val text overlap:', e)\n",
    "\n",
    "# If datasets Arrow splits exist (train_split, val_split)\n",
    "try:\n",
    "    if 'train_split' in globals() and 'val_split' in globals():\n",
    "        def ds_text_hashes(ds, text_col='review_text'):\n",
    "            seen = set()\n",
    "            for x in ds[text_col]:\n",
    "                if x is None:\n",
    "                    continue\n",
    "                seen.add(hashlib.md5(str(x).strip().encode('utf-8')).hexdigest())\n",
    "            return seen\n",
    "        s1 = ds_text_hashes(train_split)\n",
    "        s2 = ds_text_hashes(val_split)\n",
    "        results['ds_train_val_text_overlap'] = len(s1 & s2)\n",
    "        print('arrow train/val text overlap:', results['ds_train_val_text_overlap'])\n",
    "    else:\n",
    "        print('train_split/val_split not present; skipping Arrow overlap check')\n",
    "except Exception as e:\n",
    "    print('Error computing Arrow overlap:', e)\n",
    "\n",
    "# Print existing precomputed overlap variables if present\n",
    "for v in ['train_val_overlap','train_test_overlap','val_test_overlap']:\n",
    "    if v in globals():\n",
    "        print(f'{v} (existing variable) =', globals()[v])\n",
    "\n",
    "# 2) ID overlaps (product_id or any id-like column)\n",
    "try:\n",
    "    if 'train_df' in globals() and 'val_df' in globals():\n",
    "        id_cols = [c for c in train_df.columns if 'id' in c.lower() or 'product' in c.lower()]\n",
    "        for c in id_cols:\n",
    "            a = set(train_df[c].dropna().astype(str))\n",
    "            b = set(val_df[c].dropna().astype(str))\n",
    "            ov = len(a & b)\n",
    "            print(f'ID overlap on {c}:', ov)\n",
    "            results[f'id_overlap_{c}'] = ov\n",
    "    else:\n",
    "        print('train_df/val_df not available for ID overlap checks')\n",
    "except Exception as e:\n",
    "    print('Error computing ID overlaps:', e)\n",
    "\n",
    "# 3) Label distributions\n",
    "try:\n",
    "    def print_label_dist_from_df(df, name='df'):\n",
    "        vc = df['label'].value_counts(dropna=False)\n",
    "        print(f\"{name} label counts:\\n\", vc.to_dict())\n",
    "        print(f\"{name} proportions:\\n\", (vc/vc.sum()).round(3).to_dict())\n",
    "    if 'train_df' in globals():\n",
    "        print_label_dist_from_df(train_df, 'train')\n",
    "    if 'val_df' in globals():\n",
    "        print_label_dist_from_df(val_df, 'val')\n",
    "    if 'test_df' in globals():\n",
    "        print_label_dist_from_df(test_df, 'test')\n",
    "    # datasets\n",
    "    if 'train_split' in globals():\n",
    "        try:\n",
    "            print('train_split label distribution (arrow):', Counter(train_split['label']))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if 'val_split' in globals():\n",
    "        try:\n",
    "            print('val_split label distribution (arrow):', Counter(val_split['label']))\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception as e:\n",
    "    print('Error printing label distributions:', e)\n",
    "\n",
    "# 4) Prediction behavior: collapse to majority or balanced?\n",
    "try:\n",
    "    y_true = None\n",
    "    y_pred = None\n",
    "    # possible sources\n",
    "    if 'roberta_predictions' in globals() and len(roberta_predictions) > 0:\n",
    "        y_pred = np.array(roberta_predictions)\n",
    "    elif 'roberta_confidences' in globals() and len(roberta_confidences) > 0:\n",
    "        arr = np.array(roberta_confidences)\n",
    "        if arr.ndim == 2:\n",
    "            y_pred = arr.argmax(axis=1)\n",
    "    # true labels\n",
    "    if 'val_labels' in globals() and len(val_labels) > 0:\n",
    "        y_true = np.array(val_labels)\n",
    "    elif 'val_dataset' in globals():\n",
    "        try:\n",
    "            y_true = np.array(val_dataset['label'])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if y_pred is not None and y_true is not None:\n",
    "        print('pred dist:', np.bincount(y_pred))\n",
    "        print('true dist:', np.bincount(y_true))\n",
    "        print('confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "        print(classification_report(y_true, y_pred, digits=4))\n",
    "        results['prediction_collapse'] = (np.bincount(y_pred).min() == 0)\n",
    "    else:\n",
    "        print('predictions or true labels not available for prediction diagnostics')\n",
    "except Exception as e:\n",
    "    print('Error computing prediction diagnostics:', e)\n",
    "\n",
    "# 5) Chroma / retrieval DB sanity: check if collection contains val/test ids\n",
    "try:\n",
    "    if 'product_profile_collection' in globals():\n",
    "        coll = product_profile_collection\n",
    "        try:\n",
    "            # try to access metadatas via get()\n",
    "            info = coll.get()\n",
    "            metadatas = info.get('metadatas', [])\n",
    "            coll_ids = set()\n",
    "            for md in metadatas:\n",
    "                if isinstance(md, dict) and 'id' in md:\n",
    "                    coll_ids.add(str(md['id']))\n",
    "            print('Chroma collection metadata ids found:', len(coll_ids))\n",
    "            # compare with val ids if exist\n",
    "            if 'val_df' in globals():\n",
    "                val_ids = set(val_df['product_id'].dropna().astype(str)) if 'product_id' in val_df.columns else set()\n",
    "                print('overlap of Chroma collection with val_df product_id:', len(coll_ids & val_ids))\n",
    "        except Exception as e:\n",
    "            print('Could not access collection.get() result directly, trying safer paths:', e)\n",
    "    else:\n",
    "        print('product_profile_collection not in globals(); skipping Chroma checks')\n",
    "except Exception as e:\n",
    "    print('Error during Chroma checks:', e)\n",
    "\n",
    "# 6) Feature correlation checks in train_df numeric columns\n",
    "try:\n",
    "    if 'train_df' in globals():\n",
    "        num_cols = train_df.select_dtypes(include=['int','float']).columns.tolist()\n",
    "        suspicious = []\n",
    "        for c in num_cols:\n",
    "            if c == 'label':\n",
    "                continue\n",
    "            corr = train_df[c].corr(train_df['label'])\n",
    "            if pd.notna(corr) and abs(corr) > 0.6:\n",
    "                suspicious.append((c, corr))\n",
    "        if suspicious:\n",
    "            print('Highly correlated numeric features (possible leakage):', suspicious)\n",
    "        else:\n",
    "            print('No numeric features with correlation > 0.6 found in train_df')\n",
    "    else:\n",
    "        print('train_df not available for correlation checks')\n",
    "except Exception as e:\n",
    "    print('Error computing feature correlations:', e)\n",
    "\n",
    "# 7) Majority baseline\n",
    "try:\n",
    "    if 'val_df' in globals() and 'label' in val_df.columns:\n",
    "        maj = val_df['label'].mode()[0]\n",
    "        maj_acc = (val_df['label'] == maj).mean()\n",
    "        print('majority class in val:', maj, 'majority baseline accuracy:', round(maj_acc,4))\n",
    "    elif y_true is not None:\n",
    "        vals, counts = np.unique(y_true, return_counts=True)\n",
    "        maj = vals[np.argmax(counts)]\n",
    "        maj_acc = counts.max() / counts.sum()\n",
    "        print('majority class baseline (from y_true):', maj, maj_acc)\n",
    "except Exception as e:\n",
    "    print('Error computing majority baseline:', e)\n",
    "\n",
    "# 8) Quick check for tokenization/preprocessing leakage: did tokenization use full dataset?\n",
    "try:\n",
    "    # Look for tokenized_datasets or tokenizer fitted on full corpus\n",
    "    if 'tokenized_datasets' in globals():\n",
    "        print('tokenized_datasets keys:', list(tokenized_datasets.keys()))\n",
    "        # no direct proof of leakage here, just a note\n",
    "        print('Note: if tokenization/feature stats were computed on full corpus before splitting, that can leak. Check code cells where tokenizer or vectorizer is fit.')\n",
    "    else:\n",
    "        print('tokenized_datasets not present; cannot check tokenization source programmatically')\n",
    "except Exception as e:\n",
    "    print('Error checking tokenization:', e)\n",
    "\n",
    "# Synthesize a short diagnosis from checks\n",
    "diag = []\n",
    "if results.get('train_val_text_overlap', 0) > 0 or results.get('ds_train_val_text_overlap', 0) > 0:\n",
    "    diag.append('DATA LEAKAGE: exact text overlap between train and validation detected.')\n",
    "if any((results.get(k,0) > 0) for k in list(results.keys()) if k.startswith('id_overlap_')):\n",
    "    diag.append('POTENTIAL LEAKAGE: shared ids between train and validation (see id overlap counts).')\n",
    "# prediction collapse check\n",
    "if results.get('prediction_collapse', False):\n",
    "    diag.append('MODEL COLLAPSE: predictions collapse to a single class (majority). Likely severe class imbalance or improper loss/labels.')\n",
    "# class imbalance check\n",
    "try:\n",
    "    if 'train_df' in globals():\n",
    "        vc = train_df['label'].value_counts(normalize=True)\n",
    "        min_frac = vc.min()\n",
    "        if min_frac < 0.1:\n",
    "            diag.append('CLASS IMBALANCE: minority class < 10% in training set. Use class weights or sampling.')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "if not diag:\n",
    "    diag.append('No smoking-gun leakage detected by these automated checks. Next steps: check manual code cells for uses of validation/test in preprocessing or Chroma indexing. Also try class-weighted training and lower LR.')\n",
    "\n",
    "print('\\n=== CONCISE DIAGNOSIS ===')\n",
    "for d in diag:\n",
    "    print('-', d)\n",
    "\n",
    "print('\\nDiagnostics completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow-up evidence cell: print sizes, sample overlaps and prediction lengths to pinpoint leakage source\n",
    "import hashlib\n",
    "from itertools import islice\n",
    "\n",
    "print('--- Basic sizes and types ---')\n",
    "names = ['train_df','val_df','test_df','train_split','val_split','train_split_indices','val_split_indices','train_texts','val_texts','train_labels','val_labels','roberta_predictions','roberta_confidences','tokenized_datasets']\n",
    "for n in names:\n",
    "    if n in globals():\n",
    "        v = globals()[n]\n",
    "        try:\n",
    "            l = len(v)\n",
    "        except Exception:\n",
    "            l = type(v)\n",
    "        print(f\"{n}: type={type(v)}, len={l}\")\n",
    "    else:\n",
    "        print(f\"{n}: MISSING\")\n",
    "\n",
    "# If there are explicit index lists/sets, check intersections\n",
    "try:\n",
    "    if 'train_split_indices' in globals() and 'val_split_indices' in globals():\n",
    "        s1 = set(train_split_indices)\n",
    "        s2 = set(val_split_indices)\n",
    "        inter = s1 & s2\n",
    "        print('train_split_indices & val_split_indices intersection count:', len(inter))\n",
    "        if len(inter) > 0:\n",
    "            print('sample overlapping indices (up to 10):', list(islice(inter, 10)))\n",
    "except Exception as e:\n",
    "    print('Could not check index intersections:', e)\n",
    "\n",
    "# If text arrays/lists exist, show sample overlapping texts\n",
    "try:\n",
    "    if 'train_texts' in globals() and 'val_texts' in globals():\n",
    "        h1 = {hashlib.md5(t.strip().encode('utf-8')).hexdigest():t for t in train_texts if t}\n",
    "        h2 = {hashlib.md5(t.strip().encode('utf-8')).hexdigest():t for t in val_texts if t}\n",
    "        common = set(h1.keys()) & set(h2.keys())\n",
    "        print('text-hash overlap count between train_texts and val_texts:', len(common))\n",
    "        if len(common)>0:\n",
    "            print('Examples of overlapping texts (up to 5):')\n",
    "            for hh in list(common)[:5]:\n",
    "                print('-', h1[hh][:200].replace('\\n',' '))\n",
    "except Exception as e:\n",
    "    print('Could not compute overlapping texts sample:', e)\n",
    "\n",
    "# Predictions vs validation labels\n",
    "try:\n",
    "    if 'roberta_predictions' in globals():\n",
    "        print('len(roberta_predictions)=', len(roberta_predictions))\n",
    "    if 'val_labels' in globals():\n",
    "        print('len(val_labels)=', len(val_labels))\n",
    "    if 'roberta_confidences' in globals():\n",
    "        import numpy as np\n",
    "        a = np.array(roberta_confidences)\n",
    "        print('roberta_confidences shape:', a.shape)\n",
    "except Exception as e:\n",
    "    print('Error checking predictions lengths:', e)\n",
    "\n",
    "# Print precomputed overlap variables\n",
    "for v in ['train_val_overlap','train_test_overlap','val_test_overlap','train_test_overlap']:\n",
    "    if v in globals():\n",
    "        print(f'{v} =', globals()[v])\n",
    "\n",
    "print('\\nFollow-up check complete. If any overlaps > 0 above, you have data-split leakage and should rebuild splits to ensure uniqueness.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild balanced splits robustly and retrain (safer fallback behavior)\n",
    "import random\n",
    "import re\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Start: rebuild balanced dataset and retrain (robust)')\n",
    "\n",
    "# 1) locate source DataFrame\n",
    "df_candidates = ['final_df','combined_df','merged_df','reviews_clean','train_df']\n",
    "src_df = None\n",
    "for c in df_candidates:\n",
    "    if c in globals():\n",
    "        v = globals()[c]\n",
    "        if isinstance(v, pd.DataFrame) and len(v)>0:\n",
    "            src_df = v.copy()\n",
    "            src_name = c\n",
    "            break\n",
    "if src_df is None:\n",
    "    raise RuntimeError('No suitable source DataFrame found among: ' + ','.join(df_candidates))\n",
    "print('Using', src_name, 'with', len(src_df), 'rows')\n",
    "\n",
    "# 2) infer text column and label column, prefer explicit names\n",
    "text_col = None\n",
    "label_col = None\n",
    "if 'review_text' in src_df.columns:\n",
    "    text_col = 'review_text'\n",
    "else:\n",
    "    text_cols = [c for c in src_df.columns if 'review' in c.lower() or 'text' in c.lower() or 'body' in c.lower()]\n",
    "    text_col = text_cols[0] if text_cols else None\n",
    "\n",
    "if 'is_fake' in src_df.columns:\n",
    "    label_col = 'is_fake'\n",
    "else:\n",
    "    label_cols = [c for c in src_df.columns if c.lower() in ('label','is_fake','fake','target','y') or set(src_df[c].dropna().unique()).issubset({0,1})]\n",
    "    label_col = label_cols[0] if label_cols else None\n",
    "\n",
    "if text_col is None or label_col is None:\n",
    "    raise RuntimeError(f'Could not infer text or label columns. text_col={text_col}, label_col={label_col}')\n",
    "\n",
    "print('Inferred text col =', text_col, 'label col =', label_col)\n",
    "\n",
    "# Normalize labels to 0/1 where possible\n",
    "if src_df[label_col].dtype == object:\n",
    "    src_df[label_col] = src_df[label_col].astype(str).map(lambda s: 1 if re.search('fake|fraud|synthetic|bot', s, re.I) else 0)\n",
    "\n",
    "# 3) deduplicate by text and drop exact duplicates to avoid leakage\n",
    "import hashlib\n",
    "src_df['text_hash'] = src_df[text_col].fillna('').astype(str).map(lambda s: hashlib.md5(s.strip().encode('utf-8')).hexdigest())\n",
    "src_df = src_df.drop_duplicates(subset=['text_hash']).reset_index(drop=True)\n",
    "print('After dedup by text_hash:', len(src_df))\n",
    "\n",
    "# 4) ensure there are both classes — if not, synthesize fake examples by augmenting real texts\n",
    "counts = src_df[label_col].value_counts()\n",
    "print('Counts before synthetic balancing:\\n', counts)\n",
    "if len(counts) < 2 or counts.min() == 0:\n",
    "    # Synthesize fake examples if missing\n",
    "    print('Warning: only one class present or minority class missing; synthesizing fake examples from real texts')\n",
    "    real_df = src_df[src_df[label_col]==0].copy()\n",
    "    if real_df.empty:\n",
    "        raise RuntimeError('No real examples to synthesize from')\n",
    "    needed = max(1, int(0.1 * len(real_df)))\n",
    "    def augment_text_simple_local(s):\n",
    "        if not isinstance(s, str) or len(s.strip())==0:\n",
    "            return s\n",
    "        sentences = re.split(r'(?<=[.!?]) +', s)\n",
    "        if len(sentences) > 1 and random.random() < 0.5:\n",
    "            random.shuffle(sentences)\n",
    "            return ' '.join(sentences)\n",
    "        words = s.split()\n",
    "        if len(words) > 6 and random.random() < 0.5:\n",
    "            i, j = random.sample(range(len(words)), 2)\n",
    "            words[i], words[j] = words[j], words[i]\n",
    "            return ' '.join(words)\n",
    "        return s + ' ' + random.choice(['Great product.', 'Would buy again.', 'Works as expected.'])\n",
    "    synth_texts = [augment_text_simple_local(t) for t in real_df[text_col].sample(n=needed, replace=True, random_state=42).tolist()]\n",
    "    synth_df = pd.DataFrame({text_col: synth_texts, label_col: [1]*len(synth_texts)})\n",
    "    src_df = pd.concat([src_df, synth_df], ignore_index=True)\n",
    "    print('After synthesis, class counts:', src_df[label_col].value_counts().to_dict())\n",
    "\n",
    "# 5) build pool and split into train/val (80/20) with stratify\n",
    "pool_df = src_df[[text_col, label_col]].rename(columns={text_col:'text', label_col:'label'}).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print('Pool size:', len(pool_df), 'class counts:', pool_df['label'].value_counts().to_dict())\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_pool, val_pool = train_test_split(pool_df, test_size=0.2, stratify=pool_df['label'], random_state=42)\n",
    "print('Train pool size:', len(train_pool), 'Val pool size:', len(val_pool))\n",
    "\n",
    "# 6) Build HuggingFace Datasets and tokenize\n",
    "if 'tokenizer' not in globals():\n",
    "    raise RuntimeError('No tokenizer found in notebook environment. Please load tokenizer before running this cell.')\n",
    "\n",
    "hf_train = Dataset.from_pandas(train_pool.rename(columns={'text':'text','label':'label'}))\n",
    "hf_val = Dataset.from_pandas(val_pool.rename(columns={'text':'text','label':'label'}))\n",
    "\n",
    "hf_train = hf_train.map(lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
    "hf_val = hf_val.map(lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
    "\n",
    "hf_train.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "hf_val.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "\n",
    "# 7) compute class weights\n",
    "y_train = np.array(hf_train['label'])\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print('Class weights:', dict(zip(classes.tolist(), class_weights.tolist())))\n",
    "\n",
    "# 8) Weighted Trainer (compat accepts extra kwargs and handles labels under 'labels' or 'label')\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get('labels') if 'labels' in inputs else inputs.get('label')\n",
    "        outputs = model(input_ids=inputs.get('input_ids'), attention_mask=inputs.get('attention_mask'))\n",
    "        logits = outputs.logits\n",
    "        if labels is None:\n",
    "            # fallback to model-provided loss\n",
    "            loss = getattr(outputs, 'loss', torch.tensor(0.0, device=next(model.parameters()).device))\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "            loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 9) training arguments (compatibility-safe)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_balanced_check',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "def compute_metrics_small(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    f1 = f1_score(labels, preds, average='binary')\n",
    "    precision = precision_score(labels, preds, zero_division=0)\n",
    "    recall = recall_score(labels, preds, zero_division=0)\n",
    "    return {'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# 10) load or reuse model\n",
    "if 'model' not in globals():\n",
    "    if 'model_dir' in globals():\n",
    "        from transformers import RobertaForSequenceClassification\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
    "    else:\n",
    "        raise RuntimeError('No model or model_dir found in the notebook environment.')\n",
    "\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    compute_metrics=compute_metrics_small,\n",
    ")\n",
    "\n",
    "print('Starting short retraining (1 epoch) for balanced check...')\n",
    "train_result = trainer.train()\n",
    "print('Training completed. Metrics:')\n",
    "print(train_result.metrics)\n",
    "\n",
    "# Evaluate\n",
    "eval_out = trainer.predict(hf_val)\n",
    "logits = eval_out.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = eval_out.label_ids\n",
    "print('Confusion matrix:\\n', confusion_matrix(labels, preds))\n",
    "print('Classification report:\\n', classification_report(labels, preds, digits=4))\n",
    "\n",
    "# Save small balanced splits\n",
    "train_pool.to_csv('./balanced_train_pool.csv', index=False)\n",
    "val_pool.to_csv('./balanced_val_pool.csv', index=False)\n",
    "print('Saved balanced_train_pool.csv and balanced_val_pool.csv')\n",
    "\n",
    "print('Retrain cell finished (robust path)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspect cell: show columns, sample rows, and sizes of fake/real lists to choose correct text/label columns\n",
    "import pandas as pd\n",
    "import inspect\n",
    "\n",
    "candidates = ['combined_df','merged_df','final_df','reviews_clean','train_df']\n",
    "for name in candidates:\n",
    "    if name in globals():\n",
    "        df = globals()[name]\n",
    "        if isinstance(df, pd.DataFrame):\n",
    "            print('---', name, '---')\n",
    "            print('shape:', df.shape)\n",
    "            print('columns:', df.columns.tolist())\n",
    "            # show up to first 5 rows but only string/object cols and numeric cols that look like labels\n",
    "            obj_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "            sample_cols = obj_cols[:5]\n",
    "            if 'label' in df.columns:\n",
    "                sample_cols = list(dict.fromkeys(['label'] + sample_cols))\n",
    "            print('showing first 5 rows for cols:', sample_cols)\n",
    "            display(df[sample_cols].head(5))\n",
    "            # show value counts for any likely label cols\n",
    "            possible_label_cols = [c for c in df.columns if c.lower() in ('label','is_fake','fake','target','y') or set(df[c].dropna().unique()).issubset({0,1})]\n",
    "            for lc in possible_label_cols:\n",
    "                print('value counts for', lc, '\\n', df[lc].value_counts(dropna=False).to_dict())\n",
    "\n",
    "# inspect in-memory lists if present\n",
    "for list_name in ['fake_examples','real_examples','fake_examples_list','debug_fake','debug_real']:\n",
    "    if list_name in globals():\n",
    "        v = globals()[list_name]\n",
    "        try:\n",
    "            print(list_name, 'len =', len(v))\n",
    "            print('sample (up to 3):', v[:3])\n",
    "        except Exception as e:\n",
    "            print('Could not print', list_name, e)\n",
    "\n",
    "# show tokenizer info if available\n",
    "if 'tokenizer' in globals():\n",
    "    try:\n",
    "        print('Tokenizer type:', type(tokenizer))\n",
    "        if hasattr(tokenizer, 'vocab_size'):\n",
    "            print('vocab_size:', tokenizer.vocab_size)\n",
    "    except Exception as e:\n",
    "        print('Could not inspect tokenizer:', e)\n",
    "\n",
    "# show a small sample of model and trainer\n",
    "if 'model' in globals():\n",
    "    print('Model present:', model.__class__)\n",
    "if 'trainer' in globals():\n",
    "    print('Trainer present with args:', getattr(trainer, 'args', None))\n",
    "\n",
    "print('\\nQuick inspect complete. Use these outputs to pick correct text/label columns for rebuilding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild splits explicitly using `final_df` (review_text, is_fake), create 50/50 training set, 80/20 realistic validation, tokenize, and retrain\n",
    "import random\n",
    "import re\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "print('Starting explicit rebuild + retrain using final_df')\n",
    "# 1) load final_df\n",
    "if 'final_df' not in globals():\n",
    "    raise RuntimeError('`final_df` not found in notebook namespace. It must contain `review_text` and `is_fake`.')\n",
    "fd = final_df.copy()\n",
    "print('final_df shape:', fd.shape)\n",
    "if 'review_text' not in fd.columns or 'is_fake' not in fd.columns:\n",
    "    raise RuntimeError('final_df must contain columns `review_text` and `is_fake`')\n",
    "\n",
    "# 2) basic cleaning: drop NaN texts\n",
    "fd['review_text'] = fd['review_text'].astype(str).fillna('').map(lambda s: s.strip())\n",
    "fd = fd[fd['review_text'].str.len() > 0].reset_index(drop=True)\n",
    "print('after dropping empty texts:', len(fd))\n",
    "\n",
    "# 3) build validation set of size ~20% of data with 80% real and 20% fake\n",
    "total = len(fd)\n",
    "val_size = int(0.2 * total)\n",
    "print('target val size:', val_size)\n",
    "real_pool = fd[fd['is_fake']==0].copy()\n",
    "fake_pool = fd[fd['is_fake']==1].copy()\n",
    "print('real_pool:', len(real_pool), 'fake_pool:', len(fake_pool))\n",
    "\n",
    "# compute target counts\n",
    "val_real_target = int(round(val_size * 0.8))\n",
    "val_fake_target = val_size - val_real_target\n",
    "val_real = real_pool.sample(n=min(val_real_target, len(real_pool)), random_state=42)\n",
    "val_fake = fake_pool.sample(n=min(val_fake_target, len(fake_pool)), random_state=42)\n",
    "# if not enough fake examples for val_fake_target, adjust by taking as many as available\n",
    "if len(val_fake) < val_fake_target:\n",
    "    shortage = val_fake_target - len(val_fake)\n",
    "    print('Warning: not enough fake examples for desired val composition; shortage:', shortage)\n",
    "    # reduce val_real to keep val_size constant\n",
    "    if len(val_real) > shortage:\n",
    "        val_real = val_real.sample(n=(len(val_real)-shortage), random_state=42)\n",
    "\n",
    "val_df = pd.concat([val_real, val_fake]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print('Actual val composition:', val_df['is_fake'].value_counts().to_dict())\n",
    "\n",
    "# 4) remaining df for training\n",
    "val_idx = set(val_df.index)  # these are from subsets, but we need to remove by indices in original df\n",
    "# Remove val samples from fd by text hashes to avoid accidental duplicates\n",
    "val_hashes = set(val_df['review_text'].map(lambda s: hashlib.md5(s.encode('utf-8')).hexdigest()))\n",
    "fd['text_hash'] = fd['review_text'].map(lambda s: hashlib.md5(s.encode('utf-8')).hexdigest())\n",
    "train_pool_df = fd[~fd['text_hash'].isin(val_hashes)].copy().reset_index(drop=True)\n",
    "print('Train pool after removing val hashes:', len(train_pool_df), 'class counts:', train_pool_df['is_fake'].value_counts().to_dict())\n",
    "\n",
    "# 5) create balanced training set 50/50 by oversampling minority with light augmentation\n",
    "real_train = train_pool_df[train_pool_df['is_fake']==0].copy()\n",
    "fake_train = train_pool_df[train_pool_df['is_fake']==1].copy()\n",
    "print('Available for train - real:', len(real_train), 'fake:', len(fake_train))\n",
    "\n",
    "def augment_text_simple(s):\n",
    "    if not isinstance(s, str) or len(s.strip())==0:\n",
    "        return s\n",
    "    # shuffle sentences sometimes\n",
    "    sentences = re.split(r'(?<=[.!?]) +', s)\n",
    "    if len(sentences) > 1 and random.random() < 0.4:\n",
    "        random.shuffle(sentences)\n",
    "        return ' '.join(sentences)\n",
    "    words = s.split()\n",
    "    if len(words) > 6 and random.random() < 0.4:\n",
    "        i, j = random.sample(range(len(words)), 2)\n",
    "        words[i], words[j] = words[j], words[i]\n",
    "        return ' '.join(words)\n",
    "    return s + ' '\n",
    "\n",
    "# target per-class = max(len(real_train), len(fake_train))? We want balanced, so choose target = max(len(real_train), len(fake_train))\n",
    "# but to avoid huge oversampling, cap target at available majority count\n",
    "target = max(len(real_train), len(fake_train))\n",
    "if target == 0:\n",
    "    raise RuntimeError('No data available to form training set after removing validation.')\n",
    "\n",
    "# build balanced sets\n",
    "if len(real_train) > len(fake_train):\n",
    "    # oversample fake to match real\n",
    "    needed = len(real_train) - len(fake_train)\n",
    "    if len(fake_train) > 0:\n",
    "        extra = fake_train.sample(n=needed, replace=True, random_state=42).copy()\n",
    "        # augment copies to reduce identical duplicates\n",
    "        extra['review_text'] = extra['review_text'].map(lambda s: augment_text_simple(s))\n",
    "        fake_balanced = pd.concat([fake_train, extra], ignore_index=True)\n",
    "        real_balanced = real_train\n",
    "    else:\n",
    "        # synthesize fake examples from real_train by simple augment\n",
    "        synth_texts = [augment_text_simple(t) for t in real_train['review_text'].sample(n=len(real_train), replace=True, random_state=42).tolist()]\n",
    "        fake_balanced = pd.DataFrame({'review_text': synth_texts, 'is_fake': 1})\n",
    "        real_balanced = real_train\n",
    "elif len(fake_train) > len(real_train):\n",
    "    needed = len(fake_train) - len(real_train)\n",
    "    if len(real_train) > 0:\n",
    "        extra = real_train.sample(n=needed, replace=True, random_state=42).copy()\n",
    "        extra['review_text'] = extra['review_text'].map(lambda s: augment_text_simple(s))\n",
    "        real_balanced = pd.concat([real_train, extra], ignore_index=True)\n",
    "        fake_balanced = fake_train\n",
    "else:\n",
    "    real_balanced = real_train\n",
    "    fake_balanced = fake_train\n",
    "\n",
    "train_df = pd.concat([real_balanced[['review_text','is_fake']], fake_balanced[['review_text','is_fake']]], ignore_index=True).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "print('Final train counts:', train_df['is_fake'].value_counts().to_dict())\n",
    "\n",
    "# 6) Tokenize using existing tokenizer\n",
    "if 'tokenizer' not in globals():\n",
    "    raise RuntimeError('tokenizer not found in notebook namespace; please load tokenizer before running this cell')\n",
    "\n",
    "from datasets import Dataset\n",
    "hf_train = Dataset.from_pandas(train_df.rename(columns={'review_text':'text','is_fake':'label'}))\n",
    "hf_val = Dataset.from_pandas(val_df.rename(columns={'review_text':'text','is_fake':'label'}))\n",
    "\n",
    "def tok_batch(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=256)\n",
    "\n",
    "hf_train = hf_train.map(lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
    "hf_val = hf_val.map(lambda ex: tokenizer(ex['text'], truncation=True, padding='max_length', max_length=256), batched=True)\n",
    "\n",
    "hf_train.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "hf_val.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
    "\n",
    "# 7) compute class weights (based on training distribution)\n",
    "y_train = np.array(hf_train['label'])\n",
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "print('Computed class weights:', dict(zip(classes.tolist(), class_weights.tolist())))\n",
    "\n",
    "# 8) Weighted Trainer subclass\n",
    "class WeightedTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get('label')\n",
    "        outputs = model(input_ids=inputs.get('input_ids'), attention_mask=inputs.get('attention_mask'))\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# 9) Prepare model and training args\n",
    "if 'model' not in globals():\n",
    "    if 'model_dir' in globals():\n",
    "        from transformers import RobertaForSequenceClassification\n",
    "        model = RobertaForSequenceClassification.from_pretrained(model_dir, num_labels=2)\n",
    "    else:\n",
    "        raise RuntimeError('No model or model_dir present in environment')\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_balanced_finetune',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='no',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=False,\n",
    ")\n",
    "\n",
    "# 10) compute_metrics\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'f1': f1_score(labels, preds, average='binary'),\n",
    "        'precision': precision_score(labels, preds, zero_division=0),\n",
    "        'recall': recall_score(labels, preds, zero_division=0)\n",
    "    }\n",
    "\n",
    "# 11) instantiate trainer and train\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print('Begin training...')\n",
    "train_result = trainer.train()\n",
    "print('Training finished, metrics:', train_result.metrics)\n",
    "\n",
    "# 12) evaluate and print detailed report\n",
    "pred_out = trainer.predict(hf_val)\n",
    "logits = pred_out.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_out.label_ids\n",
    "print('Validation confusion matrix:\\n', confusion_matrix(labels, preds))\n",
    "print('Validation classification report:\\n', classification_report(labels, preds, digits=4))\n",
    "\n",
    "# 13) update kernel variables for later steps\n",
    "final_train_df = train_df\n",
    "final_val_df = val_df\n",
    "\n",
    "print('Retrain cell complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compatibility cell: create TrainingArguments without unsupported kwargs and run training (uses hf_train, hf_val, model, class_weights_tensor from kernel)\n",
    "from transformers import TrainingArguments\n",
    "print('Creating simpler TrainingArguments for compatibility...')\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./roberta_balanced_finetune',\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    ")\n",
    "\n",
    "# compute_metrics should exist from previous cell; if not, define it\n",
    "try:\n",
    "    compute_metrics\n",
    "except NameError:\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    def compute_metrics(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        preds = np.argmax(logits, axis=-1)\n",
    "        return {\n",
    "            'f1': f1_score(labels, preds, average='binary'),\n",
    "            'precision': precision_score(labels, preds, zero_division=0),\n",
    "            'recall': recall_score(labels, preds, zero_division=0)\n",
    "        }\n",
    "\n",
    "# instantiate trainer and train\n",
    "trainer = WeightedTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "print('Starting training (compatibility run)...')\n",
    "train_result = trainer.train()\n",
    "print('Training done. Metrics:', train_result.metrics)\n",
    "\n",
    "# evaluate\n",
    "pred_out = trainer.predict(hf_val)\n",
    "logits = pred_out.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_out.label_ids\n",
    "print('Validation confusion matrix:\\n', confusion_matrix(labels, preds))\n",
    "print('Validation classification report:\\n', classification_report(labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix: define a Trainer compute_loss compatible with extra kwargs passed by Trainer (accept **kwargs)\n",
    "class WeightedTrainerCompat(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get('label')\n",
    "        outputs = model(input_ids=inputs.get('input_ids'), attention_mask=inputs.get('attention_mask'))\n",
    "        logits = outputs.logits\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# instantiate and train using the compatibility trainer\n",
    "trainer = WeightedTrainerCompat(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "print('Starting compatible training run...')\n",
    "train_result = trainer.train()\n",
    "print('Training finished, metrics:', train_result.metrics)\n",
    "\n",
    "# evaluate\n",
    "pred_out = trainer.predict(hf_val)\n",
    "logits = pred_out.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_out.label_ids\n",
    "print('Validation confusion matrix:\\n', confusion_matrix(labels, preds))\n",
    "print('Validation classification report:\\n', classification_report(labels, preds, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 2: robust compute_loss that accepts either 'labels' or 'label' and handles None\n",
    "class WeightedTrainerCompat2(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # get labels under either key\n",
    "        labels = inputs.get('labels') if 'labels' in inputs else inputs.get('label')\n",
    "        # move tensors to device if needed (Trainer will have already moved inputs)\n",
    "        outputs = model(input_ids=inputs.get('input_ids'), attention_mask=inputs.get('attention_mask'))\n",
    "        logits = outputs.logits\n",
    "        if labels is None:\n",
    "            # fallback to model's own loss if labels absent\n",
    "            return outputs.loss if return_outputs else outputs.loss\n",
    "        loss_fct = nn.CrossEntropyLoss(weight=class_weights_tensor.to(logits.device))\n",
    "        loss = loss_fct(logits.view(-1, model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "trainer = WeightedTrainerCompat2(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_val,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "print('Starting robust compatible training run...')\n",
    "train_result = trainer.train()\n",
    "print('Training finished, metrics:', train_result.metrics)\n",
    "\n",
    "pred_out = trainer.predict(hf_val)\n",
    "logits = pred_out.predictions\n",
    "preds = np.argmax(logits, axis=1)\n",
    "labels = pred_out.label_ids\n",
    "print('Validation confusion matrix:\\n', confusion_matrix(labels, preds))\n",
    "print('Validation classification report:\\n', classification_report(labels, preds, digits=4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "education",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0be5c9295b9f49df81aa05c3e72f47ba": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1a5bc480ce3142699d66e36d4f43ff25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_69887a242dd34f32ae76a494e0522576",
      "placeholder": "​",
      "style": "IPY_MODEL_deaf405868ff4fd4879012cb63bab506",
      "value": " 266/266 [13:29&lt;00:00,  2.43it/s]"
     }
    },
    "20f2091182a8469eb37042a26d7f2a5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aca6445a37aa485691d33e40b656ec1d",
      "placeholder": "​",
      "style": "IPY_MODEL_e4ba6c56a1ba4e82b5e4b23c818a1e39",
      "value": " 1/1 [00:00&lt;00:00, 10.46it/s]"
     }
    },
    "233d7bea6fd742dda8c7e57ae7b20a2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "32b9a4d7592d46c79d227b388565411d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e8ac4c45fb314825bfe6b6ff54159f9f",
       "IPY_MODEL_4a71634b647d4724bacec1f2460bf355",
       "IPY_MODEL_1a5bc480ce3142699d66e36d4f43ff25"
      ],
      "layout": "IPY_MODEL_56e5d0f405294f48b595a7faaf2f26c4"
     }
    },
    "4a71634b647d4724bacec1f2460bf355": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0be5c9295b9f49df81aa05c3e72f47ba",
      "max": 266,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_75cb356392b24de693cbc4879b8371a9",
      "value": 266
     }
    },
    "56e5d0f405294f48b595a7faaf2f26c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5acf92b9b7084df8992f4afc51c3abc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_84e64e02f66c49038a77f028ea64b8d0",
       "IPY_MODEL_c75b57a0172043c6be2f73486c30c96d",
       "IPY_MODEL_20f2091182a8469eb37042a26d7f2a5f"
      ],
      "layout": "IPY_MODEL_233d7bea6fd742dda8c7e57ae7b20a2b"
     }
    },
    "69887a242dd34f32ae76a494e0522576": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "75cb356392b24de693cbc4879b8371a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "84e64e02f66c49038a77f028ea64b8d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9a5a4375ab984480bb40bfc5046cac3c",
      "placeholder": "​",
      "style": "IPY_MODEL_d093f56774ca41058d4d11c7042d7036",
      "value": "Batches: 100%"
     }
    },
    "88b0d89c7eb74177babbf32219df2353": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a5a4375ab984480bb40bfc5046cac3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a7e6e17c7ede401baaefaf21731b281d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "aca6445a37aa485691d33e40b656ec1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3f9d070cd9648f780e07bba1e51acb1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c75b57a0172043c6be2f73486c30c96d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a7e6e17c7ede401baaefaf21731b281d",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ca941f1e7f7646e08961853b30462e99",
      "value": 1
     }
    },
    "ca941f1e7f7646e08961853b30462e99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d093f56774ca41058d4d11c7042d7036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "deaf405868ff4fd4879012cb63bab506": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4ba6c56a1ba4e82b5e4b23c818a1e39": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e8ac4c45fb314825bfe6b6ff54159f9f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_88b0d89c7eb74177babbf32219df2353",
      "placeholder": "​",
      "style": "IPY_MODEL_c3f9d070cd9648f780e07bba1e51acb1",
      "value": "Batches: 100%"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
